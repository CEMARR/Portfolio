{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Initial Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object in this notebook is to explore the corpora that I have cleaned manually, including cleaning and engineering via python. I want to have a sense of the texts digitally as data structures as well as as texts.\n",
    "\n",
    "I will work on one text to get all the code squared away, importing other corpora as necessary. The modeling would rather require that these all be in data frames, each one in a dataframe, with the corpora split up as necessary in each one. Yikes. There is more to do than I thought, I think.\n",
    "\n",
    "Twain = 1 \n",
    "Wilde = 2 \n",
    "Lincoln = 3 \n",
    "D_Twain = 10 \n",
    "D_Wilde = 20 \n",
    "D_Lincoln = 30\n",
    "Modern = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import codecs\n",
    "import nltk\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_root = '../corpora/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = open(corpus_root +'_CLEAN_Twain.txt', 'rU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_t = t.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_t[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(raw_t)\n",
    "# 14,712,397 characters, which presumably includes the \\n's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## New line is as much a function of printing as anything else.\n",
    "## The newline characters need to be removed in order to have a clean-ish dataset.\n",
    "clean_t = raw_t.replace(\"\\n\", \" \")\n",
    "chars_t = len(clean_t)\n",
    "## 14,712,397 characters, post cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_t[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_t = nltk.word_tokenize(clean_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3143540"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_t = len(words_t)\n",
    "wc_t\n",
    "# 3,143,540 words and punctuation after replacing the 'newline' character with \" \". We will ultimately be looking at \n",
    "# n-grams, to do LSI/LSA so the punctuation is of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_t[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sents_t = nltk.sent_tokenize(clean_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sents_t[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136199"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_t = len(sents_t)\n",
    "sc_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twain_counts = ['Mark Twain', chars_t, wc_t, sc_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twain_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to have a presentation dataset for my three writers containing the name, code, and char, word, and sentence count for each corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = open(corpus_root + '_CLEAN_Lincoln.txt', 'rU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # An attempt at writing a function to manually clean up the files. It didn't work because of encoding difficulties.\n",
    "# clean_text = []\n",
    "# def text_clean(text):\n",
    "#     raw = text.read()\n",
    "#     clean = raw.replace(\"\\n\", \" \")\n",
    "#     chars = len(clean)\n",
    "#     clean_text.append(chars)\n",
    "#     words = nltk.word_tokenize(clean)\n",
    "#     wc = len(words)\n",
    "#     clean_text.append(wc)\n",
    "#     sents = nltk.sent_tokenize(clean)\n",
    "#     sc = len(sents)\n",
    "#     clean_text.append(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_c = c.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_c = raw_c.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars_c = len(clean_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_c = nltk.word_tokenize(clean_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wc_c = len(words_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents_c = nltk.sent_tokenize(clean_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc_c = len(sents_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lincoln_counts = [\"Abraham Lincoln\", chars_c, wc_c, sc_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "St. Oscar of Wilde posed a unique problem insofar as the encodings were not the same across the documents I used to create the corpus. A great deal of work had to be done in order to first figure out what had to be done and second to do it. The first block of code represents about a day's worth of work trying to solve the problem. The second represents the successful approach. If we fail in this, we shall trace it back to Saturday, 15 October AD 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## The unsuccessful approach\n",
    "# import re\n",
    "# w1 = codecs.open('../Wilde/_USED/America.txt', encoding='utf-8')\n",
    "# w2 = codecs.open('../Wilde/_USED/Aphorisms.txt')\n",
    "# ISO-646-US (US-ASCII)\n",
    "# raw_w = w1.read()\n",
    "# raw_w[:100]\n",
    "# raw_2 = w2.read()\n",
    "# raw_2[:100]\n",
    "# w3 = codecs.open('../Wilde/_USED/Essays and Lectures.txt', encoding='UTF-8-sig', errors='replace')\n",
    "# raw_3 = w3.read()\n",
    "# raw_3[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The successful approach\n",
    "w = codecs.open(corpus_root + '_CLEAN_Wilde.txt', encoding='UTF-8-sig', errors='replace' )\n",
    "w_raw = w.read()\n",
    "w_raw = w_raw.encode('ascii', 'replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_w = w_raw.replace(\"?s\", \"'s\")\n",
    "clean_w = clean_w.replace(\"\\r\", \"\")\n",
    "clean_w = clean_w.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chars_w = len(clean_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_w = nltk.word_tokenize(clean_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wc_w = len(words_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents_w = nltk.sent_tokenize(clean_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc_w = len(sents_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wilde_counts = ['Oscar Wilde', chars_w, wc_w, sc_w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the modern Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = codecs.open(corpus_root + 'mod_text.txt', encoding='utf-8-sig', errors='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m_raw = m.read()\n",
    "m_raw = m_raw.encode('ascii', 'replace')\n",
    "#m_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_m = m_raw.replace(r\"\\?{2,}\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(clean_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362171"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_m = len(clean_m)\n",
    "chars_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_m = nltk.word_tokenize(clean_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69327"
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_m = len(words_m)\n",
    "wc_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5174"
      ]
     },
     "execution_count": 657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_m = nltk.sent_tokenize(clean_m)\n",
    "sc_m = len(sents_m)\n",
    "sc_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_1k_H = int(chars_m/1000)\n",
    "m_1k_W = int(wc_m/1000)\n",
    "m_1k_S = int(sc_m/1000)\n",
    "\n",
    "m_500_H = int(chars_m/500)\n",
    "m_500_W = int(wc_m/500)\n",
    "m_500_S = int(sc_m/500)\n",
    "\n",
    "m_100_H = int(chars_m/100)\n",
    "m_100_W = int(wc_m/100)\n",
    "m_100_S = int(sc_m/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st_m = ['clean_m', 'words_m', 'sents_m']*3\n",
    "leng_m = [m_1k_H, m_1k_W, m_1k_S, m_500_H, m_500_W, m_500_S, m_100_H, m_100_W, m_100_S]\n",
    "labels_m = [\"m_1k_H\", \"m_1k_W\", \"m_1k_S\", \"m_500_H\", \"m_500_W\", \"m_500_S\", \"m_100_H\", \"m_100_W\", \"m_100_S\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On to the Good Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to have a simple presentation dataframe to keep track of the various counts here. Indices would also serve as the decoding codes for the authors, as they'll have to be decoded eventually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_list = [twain_counts, wilde_counts, lincoln_counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "authors = []\n",
    "for name in counts_list:\n",
    "    authors.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a_df = pd.DataFrame(authors, columns=['Name', 'char_count', 'word_count', 'sentence_count'], index=[1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>14712397</td>\n",
       "      <td>3143540</td>\n",
       "      <td>136199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oscar Wilde</td>\n",
       "      <td>2856299</td>\n",
       "      <td>595302</td>\n",
       "      <td>38528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abraham Lincoln</td>\n",
       "      <td>2646227</td>\n",
       "      <td>531241</td>\n",
       "      <td>17649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name  char_count  word_count  sentence_count\n",
       "1       Mark Twain    14712397     3143540          136199\n",
       "2      Oscar Wilde     2856299      595302           38528\n",
       "3  Abraham Lincoln     2646227      531241           17649"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a_df.to_csv('author_counts.csv', sep='|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A question presents itself: since we are dealing with three corpora of significantly different lengths, does it make sense to look at absolute lengths of strings or relative lengths of strings. That is, a fixed count that is the same across corpora or a percentage of the corpus at hand?\n",
    "\n",
    "My hunch would be that a relative proportion of characters would work best for identifying, but we are interested in short strings, thus strings of absolute rather than relative length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The built-in NLP libraries handle a lot of the tokenizing, but we do need to figure out the correct n-gram level for distinguishing among these three writers. The thinking, really, is to have a range of ngrams, run logistic regressions on them to see which has the best classifying against itself. (This is because we're looking to evaluate  substrings of various length, rather than full corpora.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For reference, the storage variables for the pieces of these documents:\n",
    "# Twain:\n",
    "# clean_t = characters - \n",
    "# words_t = words, tokenized\n",
    "# sents_t = sentences, tokenized\n",
    "###\n",
    "# Wilde:\n",
    "# clean_w = characters\n",
    "# words_w = words, tokenized\n",
    "# sents_w = sentences, tokenized\n",
    "###\n",
    "# Lincoln:\n",
    "# clean_c = characters\n",
    "# words_c = words, tokenized\n",
    "# sents_c = sentences, tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_df['char_len'] = (a_df['char_count']/1000)\n",
    "a_df['word_len'] = a_df['word_count']/1000\n",
    "a_df['sent_len'] = a_df['sentence_count']/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_df['char_len_D'] = a_df['char_count']/500\n",
    "a_df['word_len_D'] = a_df['word_count']/500\n",
    "a_df['sent_len_D'] = a_df['sentence_count']/500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a_df['char_len_C'] = a_df['char_count']/100\n",
    "a_df['word_len_C'] = a_df['word_count']/100\n",
    "a_df['sent_len_C'] = a_df['sentence_count']/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>char_len</th>\n",
       "      <th>word_len</th>\n",
       "      <th>sent_len</th>\n",
       "      <th>char_len_D</th>\n",
       "      <th>word_len_D</th>\n",
       "      <th>sent_len_D</th>\n",
       "      <th>char_len_C</th>\n",
       "      <th>word_len_C</th>\n",
       "      <th>sent_len_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>14712397</td>\n",
       "      <td>3143540</td>\n",
       "      <td>136199</td>\n",
       "      <td>14712.397</td>\n",
       "      <td>3143.540</td>\n",
       "      <td>136.199</td>\n",
       "      <td>29424.794</td>\n",
       "      <td>6287.080</td>\n",
       "      <td>272.398</td>\n",
       "      <td>147123.97</td>\n",
       "      <td>31435.40</td>\n",
       "      <td>1361.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oscar Wilde</td>\n",
       "      <td>2856299</td>\n",
       "      <td>595302</td>\n",
       "      <td>38528</td>\n",
       "      <td>2856.299</td>\n",
       "      <td>595.302</td>\n",
       "      <td>38.528</td>\n",
       "      <td>5712.598</td>\n",
       "      <td>1190.604</td>\n",
       "      <td>77.056</td>\n",
       "      <td>28562.99</td>\n",
       "      <td>5953.02</td>\n",
       "      <td>385.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abraham Lincoln</td>\n",
       "      <td>2646227</td>\n",
       "      <td>531241</td>\n",
       "      <td>17649</td>\n",
       "      <td>2646.227</td>\n",
       "      <td>531.241</td>\n",
       "      <td>17.649</td>\n",
       "      <td>5292.454</td>\n",
       "      <td>1062.482</td>\n",
       "      <td>35.298</td>\n",
       "      <td>26462.27</td>\n",
       "      <td>5312.41</td>\n",
       "      <td>176.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name  char_count  word_count  sentence_count   char_len  \\\n",
       "1       Mark Twain    14712397     3143540          136199  14712.397   \n",
       "2      Oscar Wilde     2856299      595302           38528   2856.299   \n",
       "3  Abraham Lincoln     2646227      531241           17649   2646.227   \n",
       "\n",
       "   word_len  sent_len  char_len_D  word_len_D  sent_len_D  char_len_C  \\\n",
       "1  3143.540   136.199   29424.794    6287.080     272.398   147123.97   \n",
       "2   595.302    38.528    5712.598    1190.604      77.056    28562.99   \n",
       "3   531.241    17.649    5292.454    1062.482      35.298    26462.27   \n",
       "\n",
       "   word_len_C  sent_len_C  \n",
       "1    31435.40     1361.99  \n",
       "2     5953.02      385.28  \n",
       "3     5312.41      176.49  "
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "leng_t = a_df.iloc[0,4:13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_w = a_df.iloc[1,4:13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_c = a_df.iloc[2,4:13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "length_t = []\n",
    "for i in leng_t:\n",
    "    length_t.append(int(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length_w = []\n",
    "for i in l_w:\n",
    "    length_w.append(int(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length_c = []\n",
    "for i in l_c:\n",
    "    length_c.append(int(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14712, 3143, 136, 29424, 6287, 272, 147123, 31435, 1361]"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2856, 595, 38, 5712, 1190, 77, 28562, 5953, 385]"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2646, 531, 17, 5292, 1062, 35, 26462, 5312, 176]"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twain = 1 \n",
    "Wilde = 2 \n",
    "Lincoln = 3 \n",
    "D_Twain = 10 \n",
    "D_Wilde = 20 \n",
    "D_Lincoln = 30\n",
    "Modern = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "#sns.distplot(a_df['char_count'])\n",
    "#sns.distplot(a_df['word_count'])\n",
    "#sns.distplot(a_df['sentence_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def var_gram(text, pct):\n",
    "    t = len(text)\n",
    "    print \"The character ngram degree for this text at \", pct, \"percent is:\"\n",
    "    pct = pct/100.0\n",
    "    print int(pct * t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var_gram(clean_t, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A relative length n-grammer\n",
    "def n_grammr(text, n):\n",
    "    gram = []\n",
    "    p = int(n*len(text))\n",
    "    c = [text[0+i:p+i] for i in range(0, len(text), p)]\n",
    "    gram.append(c)\n",
    "    df = pd.DataFrame(gram).T\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LE JARDIN.      The lily's withered chalice fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>even told her own mother.  I don't know what ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>es must have been partially naval, ?for Agamem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d, fogs are carried to excess.  They have beco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lyre,' and the 'famous final victory,' in such...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ce.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  LE JARDIN.      The lily's withered chalice fa...\n",
       "1   even told her own mother.  I don't know what ...\n",
       "2  es must have been partially naval, ?for Agamem...\n",
       "3  d, fogs are carried to excess.  They have beco...\n",
       "4  lyre,' and the 'famous final victory,' in such...\n",
       "5                                               ce. "
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_grammr(clean_w, .20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_grammr_abs(text, n):\n",
    "    gram = []\n",
    "    c = [text[0+i:n+i] for i in range(0, len(text), n)]\n",
    "    gram.append(c)\n",
    "    df = pd.DataFrame(gram).T\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# two n_grammrs set up: one for absolute length, one for proportional\n",
    "def framer(string, n, tpe, auth):\n",
    "    frame = n_grammr_abs(string, n)\n",
    "    if auth == 't':\n",
    "        frame['code'] = 1\n",
    "    elif auth == 'w':\n",
    "        frame['code'] = 2\n",
    "    elif auth == 'c':\n",
    "        frame['code'] = 3\n",
    "    elif auth == 'm':\n",
    "        frame['code'] = 100\n",
    "    filename = \"{}_{}{}.csv\".format(auth, n, tpe)\n",
    "    frame.to_csv(filename, sep=\"|\", encoding=\"utf-8\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This was a lovely function that killed the kernel a few times... \n",
    "    \n",
    "# def n_grammr(text, n):\n",
    "#     longstring = itertools.chain(text)\n",
    "#     laststring = ''\n",
    "#     for item in longstring: \n",
    "#         laststring += item\n",
    "#     for i in range(5, len(laststring)+1):\n",
    "#         gram.append(laststring[i-n:i])\n",
    "#     df = pd.DataFrame(gram)\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Model code to set up a variable ngram generator, as above. Original version killed the kernel\n",
    "\n",
    "# #string = ['penguin ', 'babboon ', 'puffin ']\n",
    "# string = ['penguin babboon puffin']\n",
    "# import itertools \n",
    "\n",
    "# longstring = itertools.chain(string)\n",
    "\n",
    "# laststring = ''\n",
    "\n",
    "# for item in longstring: \n",
    "#     laststring += item\n",
    "    \n",
    "# for i in range(5, len(laststring)+1):\n",
    "#     print laststring[i-5:i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginning of procedure for automated 'chunking' of the string sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_grammr_abs(text, n):\n",
    "    gram = []\n",
    "    c = [text[0+i:n+i] for i in range(0, len(text), n)]\n",
    "    gram.append(c)\n",
    "    df = pd.DataFrame(gram).T\n",
    "    return df\n",
    "\n",
    "# two n_grammrs set up: one for absolute length, one for proportional\n",
    "def framer(string, n, tpe, auth):\n",
    "    frame = n_grammr_abs(string, n)\n",
    "    if auth == 't':\n",
    "        frame['code'] = 1\n",
    "    elif auth == 'w':\n",
    "        frame['code'] = 2\n",
    "    elif auth == 'c':\n",
    "        frame['code'] = 3\n",
    "    filename = \"{}_{}{}.csv\".format(auth, n, tpe)\n",
    "    frame.to_csv(filename, sep=\"|\", encoding=\"utf-8\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>char_len</th>\n",
       "      <th>word_len</th>\n",
       "      <th>sent_len</th>\n",
       "      <th>char_len_D</th>\n",
       "      <th>word_len_D</th>\n",
       "      <th>sent_len_D</th>\n",
       "      <th>char_len_C</th>\n",
       "      <th>word_len_C</th>\n",
       "      <th>sent_len_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>14712397</td>\n",
       "      <td>3143540</td>\n",
       "      <td>136199</td>\n",
       "      <td>14712.397</td>\n",
       "      <td>3143.540</td>\n",
       "      <td>136.199</td>\n",
       "      <td>29424.794</td>\n",
       "      <td>6287.080</td>\n",
       "      <td>272.398</td>\n",
       "      <td>147123.97</td>\n",
       "      <td>31435.40</td>\n",
       "      <td>1361.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oscar Wilde</td>\n",
       "      <td>2856299</td>\n",
       "      <td>595302</td>\n",
       "      <td>38528</td>\n",
       "      <td>2856.299</td>\n",
       "      <td>595.302</td>\n",
       "      <td>38.528</td>\n",
       "      <td>5712.598</td>\n",
       "      <td>1190.604</td>\n",
       "      <td>77.056</td>\n",
       "      <td>28562.99</td>\n",
       "      <td>5953.02</td>\n",
       "      <td>385.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abraham Lincoln</td>\n",
       "      <td>2646227</td>\n",
       "      <td>531241</td>\n",
       "      <td>17649</td>\n",
       "      <td>2646.227</td>\n",
       "      <td>531.241</td>\n",
       "      <td>17.649</td>\n",
       "      <td>5292.454</td>\n",
       "      <td>1062.482</td>\n",
       "      <td>35.298</td>\n",
       "      <td>26462.27</td>\n",
       "      <td>5312.41</td>\n",
       "      <td>176.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name  char_count  word_count  sentence_count   char_len  \\\n",
       "1       Mark Twain    14712397     3143540          136199  14712.397   \n",
       "2      Oscar Wilde     2856299      595302           38528   2856.299   \n",
       "3  Abraham Lincoln     2646227      531241           17649   2646.227   \n",
       "\n",
       "   word_len  sent_len  char_len_D  word_len_D  sent_len_D  char_len_C  \\\n",
       "1  3143.540   136.199   29424.794    6287.080     272.398   147123.97   \n",
       "2   595.302    38.528    5712.598    1190.604      77.056    28562.99   \n",
       "3   531.241    17.649    5292.454    1062.482      35.298    26462.27   \n",
       "\n",
       "   word_len_C  sent_len_C  \n",
       "1    31435.40     1361.99  \n",
       "2     5953.02      385.28  \n",
       "3     5312.41      176.49  "
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14712, 3143, 136, 29424, 6287, 272, 147123, 31435, 1361]"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_labels = ['t_1k_H', 't_1k_W', 't_1k_S', 't_500_H', 't_500_W', 't_500_S', 't_100_H', 't_100_W', 't_100_S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2856, 595, 38, 5712, 1190, 77, 28562, 5953, 385]"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_labels = ['w_1k_H', 'w_1k_W', 'w_1k_S', 'w_500_H', 'w_500_W', 'w_500_S', 'w_100_H', 'w_100_W', 'w_100_S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2646, 531, 17, 5292, 1062, 35, 26462, 5312, 176]"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_labels = ['c_1k_H', 'c_1k_W', 'c_1k_S', 'c_500_H', 'c_500_W', 'c_500_S', 'c_100_H', 'c_100_W', 'c_100_S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st_t = strings_t*3\n",
    "st_w = strings_w*3\n",
    "st_c = strings_c*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# t_comb = zip(t_labels, st_t, length_t)\n",
    "# w_comb = zip(w_labels, st_w, length_w)\n",
    "# c_comb = zip(c_labels, st_c, length_c)\n",
    "m_comb = zip(labels_m, st_m, leng_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('t_1k_H', 'clean_t', 14712),\n",
       " ('t_1k_W', 'words_t', 3143),\n",
       " ('t_1k_S', 'sents_t', 136),\n",
       " ('t_500_H', 'clean_t', 29424),\n",
       " ('t_500_W', 'words_t', 6287),\n",
       " ('t_500_S', 'sents_t', 272),\n",
       " ('t_100_H', 'clean_t', 147123),\n",
       " ('t_100_W', 'words_t', 31435),\n",
       " ('t_100_S', 'sents_t', 1361)]"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('w_1k_H', 'clean_w', 2856),\n",
       " ('w_1k_W', 'words_w', 595),\n",
       " ('w_1k_S', 'sents_w', 38),\n",
       " ('w_500_H', 'clean_w', 5712),\n",
       " ('w_500_W', 'words_w', 1190),\n",
       " ('w_500_S', 'sents_w', 77),\n",
       " ('w_100_H', 'clean_w', 28562),\n",
       " ('w_100_W', 'words_w', 5953),\n",
       " ('w_100_S', 'sents_w', 385)]"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c_1k_H', 'clean_c', 2646),\n",
       " ('c_1k_W', 'words_c', 531),\n",
       " ('c_1k_S', 'sents_c', 17),\n",
       " ('c_500_H', 'clean_c', 5292),\n",
       " ('c_500_W', 'words_c', 1062),\n",
       " ('c_500_S', 'sents_c', 35),\n",
       " ('c_100_H', 'clean_c', 26462),\n",
       " ('c_100_W', 'words_c', 5312),\n",
       " ('c_100_S', 'sents_c', 176)]"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('m_1k_H', 'clean_m', 362),\n",
       " ('m_1k_W', 'words_m', 69),\n",
       " ('m_1k_S', 'sents_m', 5),\n",
       " ('m_500_H', 'clean_m', 724),\n",
       " ('m_500_W', 'words_m', 138),\n",
       " ('m_500_S', 'sents_m', 10),\n",
       " ('m_100_H', 'clean_m', 3621),\n",
       " ('m_100_W', 'words_m', 693),\n",
       " ('m_100_S', 'sents_m', 51)]"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Notes to self on what I'm trying to do:\n",
    "# for t_1k_C the function should call: clean_t, length of 14712"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st_t_v = {'t_1k_H':clean_t, 't_1k_W':words_t, 't_1k_S':sents_t,\n",
    "          't_500_H':clean_t, 't_500_W':words_t, 't_500_S':sents_t,\n",
    "          't_100_H':clean_t, 't_100_W':words_t, 't_100_S':sents_t}\n",
    "\n",
    "st_w_v = {'w_1k_H':clean_w, 'w_1k_W':words_w, 'w_1k_S':sents_w,\n",
    "          'w_500_H':clean_w, 'w_500_W':words_w, 'w_500_S':sents_w,\n",
    "          'w_100_H':clean_w, 'w_100_W':words_w, 'w_100_S':sents_w}\n",
    "\n",
    "st_c_v = {'c_1k_H':clean_c, 'c_1k_W':words_c, 'c_1k_S':sents_c,\n",
    "          'c_500_H':clean_c, 'c_500_W':words_c, 'c_500_S':sents_c,\n",
    "          'c_100_H':clean_c, 'c_100_W':words_c, 'c_100_S':sents_c}\n",
    "\n",
    "st_m_v = {'m_1k_H':clean_m, 'm_1k_W':words_m, 'm_1k_S':sents_m,\n",
    "          'm_500_H':clean_m, 'm_500_W':words_m, 'm_500_S':sents_m,\n",
    "          'm_100_H':clean_m, 'm_100_W': words_m, 'm_100_S': sents_m}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_grammr_abs(text, n):\n",
    "    gram = []\n",
    "    c = [text[0+i:n+i] for i in range(0, len(text), n)]\n",
    "    gram.append(c)\n",
    "    df = pd.DataFrame(gram).T\n",
    "    return df\n",
    "\n",
    "# two n_grammrs set up: one for absolute length, one for proportional\n",
    "def framer(string, n, tpe, auth):\n",
    "    frame = n_grammr_abs(string, n)\n",
    "    if auth == 't':\n",
    "        frame['code'] = 1\n",
    "    elif auth == 'w':\n",
    "        frame['code'] = 2\n",
    "    elif auth == 'c':\n",
    "        frame['code'] = 3\n",
    "    filename = \"{}_{}{}.csv\".format(auth, n, tpe)\n",
    "    frame.to_csv(filename, sep=\"|\", encoding=\"utf-8\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('t_1k_H', 'clean_t', 14712)\n",
      "t_14712clean_t.csv\n",
      "('t_1k_W', 'words_t', 3143)\n",
      "t_3143words_t.csv\n",
      "('t_1k_S', 'sents_t', 136)\n",
      "t_136sents_t.csv\n",
      "('t_500_H', 'clean_t', 29424)\n",
      "t_29424clean_t.csv\n",
      "('t_500_W', 'words_t', 6287)\n",
      "t_6287words_t.csv\n",
      "('t_500_S', 'sents_t', 272)\n",
      "t_272sents_t.csv\n",
      "('t_100_H', 'clean_t', 147123)\n",
      "t_147123clean_t.csv\n",
      "('t_100_W', 'words_t', 31435)\n",
      "t_31435words_t.csv\n",
      "('t_100_S', 'sents_t', 1361)\n",
      "t_1361sents_t.csv\n"
     ]
    }
   ],
   "source": [
    "for x in t_comb:\n",
    "    call = x[0]\n",
    "    print x\n",
    "    print framer(st_t_v[call],x[2],x[1],'t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('w_1k_H', 'clean_w', 2856)\n",
      "w_2856clean_w.csv\n",
      "('w_1k_W', 'words_w', 595)\n",
      "w_595words_w.csv\n",
      "('w_1k_S', 'sents_w', 38)\n",
      "w_38sents_w.csv\n",
      "('w_500_H', 'clean_w', 5712)\n",
      "w_5712clean_w.csv\n",
      "('w_500_W', 'words_w', 1190)\n",
      "w_1190words_w.csv\n",
      "('w_500_S', 'sents_w', 77)\n",
      "w_77sents_w.csv\n",
      "('w_100_H', 'clean_w', 28562)\n",
      "w_28562clean_w.csv\n",
      "('w_100_W', 'words_w', 5953)\n",
      "w_5953words_w.csv\n",
      "('w_100_S', 'sents_w', 385)\n",
      "w_385sents_w.csv\n"
     ]
    }
   ],
   "source": [
    "for x in w_comb:\n",
    "    call = x[0]\n",
    "    print x\n",
    "    print framer(st_w_v[call],x[2],x[1],'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('c_1k_H', 'clean_c', 2646)\n",
      "c_2646clean_c.csv\n",
      "('c_1k_W', 'words_c', 531)\n",
      "c_531words_c.csv\n",
      "('c_1k_S', 'sents_c', 17)\n",
      "c_17sents_c.csv\n",
      "('c_500_H', 'clean_c', 5292)\n",
      "c_5292clean_c.csv\n",
      "('c_500_W', 'words_c', 1062)\n",
      "c_1062words_c.csv\n",
      "('c_500_S', 'sents_c', 35)\n",
      "c_35sents_c.csv\n",
      "('c_100_H', 'clean_c', 26462)\n",
      "c_26462clean_c.csv\n",
      "('c_100_W', 'words_c', 5312)\n",
      "c_5312words_c.csv\n",
      "('c_100_S', 'sents_c', 176)\n",
      "c_176sents_c.csv\n"
     ]
    }
   ],
   "source": [
    "for x in c_comb:\n",
    "    call = x[0]\n",
    "    print x\n",
    "    print framer(st_c_v[call],x[2],x[1],'c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('m_1k_H', 'clean_m', 362)\n",
      "m_362clean_m.csv\n",
      "('m_1k_W', 'words_m', 69)\n",
      "m_69words_m.csv\n",
      "('m_1k_S', 'sents_m', 5)\n",
      "m_5sents_m.csv\n",
      "('m_500_H', 'clean_m', 724)\n",
      "m_724clean_m.csv\n",
      "('m_500_W', 'words_m', 138)\n",
      "m_138words_m.csv\n",
      "('m_500_S', 'sents_m', 10)\n",
      "m_10sents_m.csv\n",
      "('m_100_H', 'clean_m', 3621)\n",
      "m_3621clean_m.csv\n",
      "('m_100_W', 'words_m', 693)\n",
      "m_693words_m.csv\n",
      "('m_100_S', 'sents_m', 51)\n",
      "m_51sents_m.csv\n"
     ]
    }
   ],
   "source": [
    "for x in m_comb:\n",
    "    call = x[0]\n",
    "    print x\n",
    "    print framer(st_m_v[call],x[2],x[1],'m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Kept here as an example of something tried and rejected. About 12-15 hours of work is represented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# strings_w = ['clean_w', 'words_w', 'sents_w']\n",
    "# strings_w_vars = {'clean_w':clean_w, 'words_w':words_w, 'sents_w':sents_w}\n",
    "\n",
    "# strings_c = ['clean_c', 'words_c', 'sents_c']\n",
    "# strings_c_vars = {'clean_c':clean_c, 'words_c':words_c, 'sents_c':sents_c}\n",
    "\n",
    "# strings_t = ['clean_t', 'words_t', 'sents_t']\n",
    "# strings_t_vars = {'clean_t':clean_t, 'words_t':words_t, 'sents_t':sents_t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # auth = ['w', 'c']\n",
    "# # tpe = ['C', 'W', 'S']\n",
    "# # pct = [10, 20, 30, 40]\n",
    "# # pcts2 = [.10, .20, .30, .40]\n",
    "# # lengths = [1000, 5000, 10000, 20000]\n",
    "# # lengths_t = [10000, 15000, 20000, 30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# z_t= itertools.product(strings_t, length_t)\n",
    "\n",
    "# combos_t = []\n",
    "# for i,j in z_t:\n",
    "#     if 'clean' in i:\n",
    "#         t = 'c'\n",
    "#     elif 'words' in i:\n",
    "#         t = 'w'\n",
    "#     elif 'sents' in i:\n",
    "#         t = 's'\n",
    "#     combos_t.append([i,j,t])\n",
    "# combos_t\n",
    "\n",
    "# for x in combos_t:\n",
    "#     call = x[0]\n",
    "#     print x\n",
    "#     print framer(strings_t_vars[call],x[1],x[2],'t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Twain\n",
    "By Characters - 14,712,397\n",
    "\n",
    "Having done this by hand, I decided it would make a lot more sense to automate it. That decision took about 2 days to bring to fruition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_10kC = pd.DataFrame()\n",
    "\n",
    "t_10kC = n_grammr(clean_t, 10000)\n",
    "\n",
    "t_10kC.to_csv('t_10kC.csv', sep=\"|\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_50kC = pd.DataFrame()\n",
    "\n",
    "t_50kC = n_grammr(clean_t, 50000)\n",
    "\n",
    "t_50kC.to_csv('t_50kC.csv', sep=\"|\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_100kC = pd.DataFrame()\n",
    "\n",
    "t_100kC = n_grammr(clean_t, 100000)\n",
    "\n",
    "t_100kC.to_csv('t_100kC.csv', sep=\"|\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "By Words (words_t) - 3,143,540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_1kW = pd.DataFrame()\n",
    "t_1kW = n_grammr(words_t, 1000)\n",
    "\n",
    "t_1kW.to_csv('t_1kW.csv', sep='|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_10kW = pd.DataFrame()\n",
    "t_10kW = n_grammr(words_t, 10000)\n",
    "\n",
    "t_10kW.to_csv('t_10kW.csv', sep=\"|\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_25kW = pd.DataFrame()\n",
    "t_25kW = n_grammr(words_t, 25000)\n",
    "\n",
    "t_25kW.to_csv('t_25kW.csv', sep=\"|\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_50kW = pd.DataFrame()\n",
    "t_50kW = n_grammr(words_t, 50000)\n",
    "\n",
    "t_50kW.to_csv('t_50kW.csv', sep=\"|\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By sentences - 136,199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_1kS = pd.DataFrame()\n",
    "t_1kS = n_grammr(sents_t, 1000)\n",
    "\n",
    "t_1kS.to_csv('t_1kS.csv', sep=\"|\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_5kS = pd.DataFrame()\n",
    "t_5kS = n_grammr(sents_t, 5000)\n",
    "\n",
    "t_5kS.to_csv('t_5kS.csv', sep=\"|\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_10kS = pd.DataFrame()\n",
    "t_10kS = n_grammr(sents_t, 10000)\n",
    "\n",
    "t_10kS.to_csv('t_10kS.csv', sep=\"|\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a sidestep to get the wordclouds for my authors. Credit to Sam Stack and Sheena Lee Villaneuva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twain_words = pd.DataFrame(words_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twain_words = twain_words[~twain_words[0].isin(stop_words)]\n",
    "twain_words.to_csv('twain_wc_clean.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
